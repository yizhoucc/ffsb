{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/ffsb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "os.chdir('../..')\n",
    "print(os.getcwd())\n",
    "import numpy as np\n",
    "import torch\n",
    "from numpy import pi\n",
    "from matplotlib import pyplot as plt\n",
    "from firefly_utils.data_handler import data_handler\n",
    "from firefly_utils.spike_times_class import spike_counts\n",
    "from firefly_utils.behav_class import *\n",
    "from firefly_utils.lfp_class import lfp_class\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.io import loadmat\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from numpy.lib.npyio import save\n",
    "from cmaes import CMA\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import heapq\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import time\n",
    "from stable_baselines3 import TD3\n",
    "from InverseFuncs import *\n",
    "from monkey_functions import *\n",
    "from firefly_task import ffacc_real\n",
    "from env_config import Config\n",
    "from notification import notify\n",
    "from pathlib import Path\n",
    "import configparser\n",
    "from plot_ult import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('privateconfig'))\n",
    "resdir=config['Datafolder']['data']\n",
    "resdir = Path(resdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_vec(dictionary):\n",
    "    return np.hstack(list(dictionary.values()))\n",
    "\n",
    "def time_stamps_rebin(time_stamps, binwidth_ms=20):\n",
    "    rebin = {}\n",
    "    for tr in time_stamps.keys():\n",
    "        ts = time_stamps[tr]\n",
    "        tp_num = np.floor((ts[-1] - ts[0]) * 1000 / (binwidth_ms))\n",
    "        rebin[tr] = ts[0] + np.arange(tp_num) * binwidth_ms / 1000.\n",
    "    return rebin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading...\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'units', 'lfps', 'trials_behv', 'behv_stats', 'prs', '__function_workspace__'])\n",
      "unable to extract pairs, different blocks number\n",
      "no eyetracking...\n",
      "no normalized t_ptb (t_ptbn)\n",
      "['all', 'reward', 'density', 'ptb', 'microstim', 'landmark', 'replay', 'controlgain', 'firefly_fullON']\n",
      "Succesfully set filter\n"
     ]
    }
   ],
   "source": [
    "# from data handler, load the data and cut ----------------------------------\n",
    "print('start loading...')\n",
    "dat = loadmat(resdir/'neuraltest/m53s31.mat')\n",
    "\n",
    "# lfp_beta = loadmat('/Volumes/TOSHIBA EXT/dataset_firefly/lfp_beta_m53s50.mat')\n",
    "# lfp_alpha = loadmat('/Volumes/TOSHIBA EXT/dataset_firefly/lfp_alpha_m53s50.mat')\n",
    "# lfp_theta = loadmat('/Volumes/TOSHIBA EXT/dataset_firefly/lfp_theta_m53s50.mat')\n",
    "print(dat.keys())\n",
    "behav_stat_key = 'behv_stats'\n",
    "spike_key = 'units'\n",
    "behav_dat_key = 'trials_behv'\n",
    "lfp_key = 'lfps'\n",
    "\n",
    "pre_trial_dur = 0.5\n",
    "post_trial_dur = 0.5\n",
    "# exp_data = data_handler(dat,behav_dat_key,spike_key,lfp_key,behav_stat_key,pre_trial_dur=pre_trial_dur,post_trial_dur=post_trial_dur,\n",
    "#                         lfp_beta=lfp_beta['lfp_beta'],lfp_alpha=lfp_alpha['lfp_alpha'],extract_lfp_phase=True)\n",
    "exp_data = data_handler(dat, behav_dat_key, spike_key, lfp_key, behav_stat_key, pre_trial_dur=pre_trial_dur, extract_fly_and_monkey_xy=True,\n",
    "                        post_trial_dur=post_trial_dur,\n",
    "                        lfp_beta=None, lfp_alpha=None, extract_lfp_phase=True)\n",
    "\n",
    "exp_data.set_filters('all', True)\n",
    "\n",
    "# rebin to 0.1 sec\n",
    "ts = exp_data.rebin_time_stamps(0.1)\n",
    "# ts=None\n",
    "# select the stat/stop trial\n",
    "# t_targ = dict_to_vec(exp_data.behav.events.t_targ)+0.3\n",
    "t_targ = dict_to_vec(exp_data.behav.events.t_targ)\n",
    "# t_move = dict_to_vec(exp_data.behav.events.t_move)\n",
    "# t_start = np.min(np.vstack((t_targ)),axis=0) - pre_trial_dur\n",
    "t_start = t_targ\n",
    "t_stop = dict_to_vec(exp_data.behav.events.t_stop)\n",
    "\n",
    "# concatenate a cuple of variables with the 0.2 binning\n",
    "var_names = 'rad_vel', 'ang_vel', 'x_monk', 'y_monk'  # ,'t_move'\n",
    "y, X, trial_idx = exp_data.concatenate_inputs(\n",
    "    *var_names, t_start=t_start, t_stop=t_stop, time_stamps=ts)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1. ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate ts for a trial\n",
    "time = ts[np.unique(trial_idx)[0]]\n",
    "start = t_start[np.unique(trial_idx)[0]]\n",
    "stop = t_stop[np.unique(trial_idx)[0]]\n",
    "time = time[(time>=start) & (time<stop) ]\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct belief ----------------------------------------\n",
    "warnings.filterwarnings('ignore')\n",
    "torch.manual_seed(42)\n",
    "arg = Config()\n",
    "\n",
    "print('loading data')\n",
    "datapath = Path(resdir/\"neuraltest/1208pack\")\n",
    "with open(datapath, 'rb') as f:\n",
    "    states, actions, tasks = pickle.load(f)\n",
    "\n",
    "env = ffacc_real.FireFlyPaper2(arg)\n",
    "env.debug = 1\n",
    "phi = torch.tensor([[0.4],\n",
    "                    [pi/2],\n",
    "                    [0.001],\n",
    "                    [0.001],\n",
    "                    [0.001],\n",
    "                    [0.001],\n",
    "                    [0.13],\n",
    "                    [0.001],\n",
    "                    [0.001],\n",
    "                    [0.001],\n",
    "                    [0.001],\n",
    "                    ])\n",
    "agent_ = TD3.load('trained_agent/paper.zip')\n",
    "agent = agent_.actor.mu.cpu()\n",
    "\n",
    "\n",
    "invfile = Path(resdir/'neuraltest/inv_schroall_constrain_nopert_part2')\n",
    "finaltheta, finalcov, err = process_inv(\n",
    "    invfile, removegr=False, usingbest=False)\n",
    "\n",
    "\n",
    "# run the agent\n",
    "beliefs, covs = [], []\n",
    "ntrial = 1\n",
    "theta = finaltheta\n",
    "removemask = []\n",
    "for ind in range(len(tasks)):\n",
    "    if len(actions[ind]) < 5:\n",
    "        removemask.append(list(set(trial_idx))[ind])\n",
    "    else:\n",
    "        _, _, ep_beliefs, ep_covs = run_trials(agent=agent, env=env, phi=phi, theta=theta, task=tasks[ind], ntrials=ntrial,\n",
    "                                               pert=None, given_obs=None, return_belief=True, given_action=actions[ind], given_state=states[ind])\n",
    "        beliefs.append(ep_beliefs[0]-ep_beliefs[0][0])\n",
    "        covs.append(ep_covs[0])\n",
    "        assert len(ep_beliefs[0]) == len(actions[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts 40\n",
      "neural (11, 140)\n"
     ]
    }
   ],
   "source": [
    "uniquetrialidx=list(set(trial_idx))\n",
    "print('ts', len(ts[uniquetrialidx[0]]))\n",
    "\n",
    "t_start[uniquetrialidx[0]]\n",
    "t_stop[uniquetrialidx[0]]\n",
    "print('neural', (y.T[trial_idx==uniquetrialidx[0]]).shape)\n",
    "print('neural', (beliefs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post process the beliefs\n",
    "\n",
    "# remove invalid inds\n",
    "mask = [True if i not in removemask else False for i in trial_idx]\n",
    "\n",
    "res={k:v[mask] for k,v in X.items()}\n",
    "res['y']=y[:, mask].T\n",
    "res['trial_idx']=trial_idx[mask]\n",
    "b= np.vstack(beliefs)[:, :, 0].T\n",
    "res['cov']= np.vstack(covs)\n",
    "\n",
    "resdir.mkdir(parents=True, exist_ok=True)\n",
    "resdir = Path(resdir)/'neuraltest/res'\n",
    "\n",
    "# convert model units to real world units\n",
    "b[[0,1,3]]=b[[0,1,3]]*500\n",
    "b[[2,4]]=b[[2,4]]*180/pi\n",
    "res['belief']=b.T\n",
    "# do the same for cov\n",
    "\n",
    "\n",
    "\n",
    "# zero the belief start\n",
    "\n",
    "# zero the state start\n",
    "\n",
    "# add the ts\n",
    "res['ts']=all_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the \n",
    "with open(resdir/'0223_new_belief', 'wb+') as f:\n",
    "    pickle.dump(res, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
