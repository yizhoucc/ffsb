{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load agents and environments\n",
    "%run Agent_Env_concat_changed_TD3.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get configures\n",
    "arg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SEED_NUMBER': 0,\n",
       " 'ACTION_DIM': 2,\n",
       " 'STATE_DIM': 22,\n",
       " 'TERMINAL_ACTION': tensor(0.1000),\n",
       " 'DELTA_T': tensor(1),\n",
       " 'EPISODE_TIME': tensor(40),\n",
       " 'EPISODE_LEN': tensor(40),\n",
       " 'TOT_T': 2000000000,\n",
       " 'BATCH_SIZE': 512,\n",
       " 'REWARD': tensor(10),\n",
       " 'DISCOUNT_FACTOR': 0.99,\n",
       " 'SOFT_UPDATE_TAU': 0.005,\n",
       " 'STD_STEP_SIZE': 0.0001,\n",
       " 'MEMORY_SIZE': 500000.0,\n",
       " 'filename': '20200605-110526',\n",
       " 'data_path': './',\n",
       " 'goal_radius_range': tensor([0.1625, 0.1625]),\n",
       " 'initial_radius_range': [0.25, 1.0],\n",
       " 'relative_angle_range': tensor([-0.6981,  0.6981]),\n",
       " 'process_gain_range': tensor([0.0500, 0.0500, 0.1571, 0.1571]),\n",
       " 'noise_covariance_range': tensor([2.5000e-05, 2.5000e-03, 2.4674e-04, 2.4674e-02]),\n",
       " 'perturbation_velocity_range': [-0.05,\n",
       "  0.05,\n",
       "  -0.15707963267948966,\n",
       "  0.15707963267948966],\n",
       " 'perturbation_duration': 10,\n",
       " 'perturbation_std': 2,\n",
       " 'perturbation_delay_T_range': [1, 10]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arg.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed, so that results can be repeated\n",
    "random.seed(arg.SEED_NUMBER) # rand seed\n",
    "torch.manual_seed(arg.SEED_NUMBER) # cpu seed\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(arg.SEED_NUMBER) # gpu seed\n",
    "np.random.seed(arg.SEED_NUMBER) # numpy random seed\n",
    "\n",
    "torch.backends.cudnn.deterministic = True # only deterministic cudnn algorithms\n",
    "torch.backends.cudnn.benchmark = True # not choose the fastest cudnn algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "# if gpu is to be used\n",
    "CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize environment \n",
    "env = Model(arg, enable_perturbation=False, enable_radius_staircase = False)\n",
    "# reset environment for the first trial\n",
    "x, target_position, isPerturbation, pro_gains, pro_noise_covariance, goal_radius = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save configures\n",
    "filename = arg.filename\n",
    "#filename = '20200602-183506'\n",
    "\n",
    "argument = arg.__dict__\n",
    "torch.save(argument, './trained_agent/'+filename+'_arg_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize agent and reset the agent for the first trial\n",
    "agent = Agent(arg)\n",
    "#agent.load(filename)\n",
    "b_v, b_x, state = agent.Bstep.reset(pro_gains, pro_noise_covariance, goal_radius, target_position, \n",
    "                                                isPerturbation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define action space noise\n",
    "action_std = torch.tensor(0.20)\n",
    "noise = ActionNoise(arg.ACTION_DIM, mean=torch.tensor(0), std=action_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 9803.0, Ep: 499.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 19.645, mean reward: 0.074, mean reward per step: 0.004, rewarded fraction: 0.044\n",
      "t: 20563.0, Ep: 999.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 21.520, mean reward: 5.128, mean reward per step: 0.238, rewarded fraction: 0.492\n",
      "t: 31631.0, Ep: 1499.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 22.136, mean reward: 8.635, mean reward per step: 0.390, rewarded fraction: 0.618\n",
      "t: 42878.0, Ep: 1999.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 22.494, mean reward: 8.713, mean reward per step: 0.387, rewarded fraction: 0.630\n",
      "t: 53900.0, Ep: 2499.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 22.044, mean reward: 8.502, mean reward per step: 0.386, rewarded fraction: 0.646\n",
      "t: 64148.0, Ep: 2999.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 20.496, mean reward: 8.709, mean reward per step: 0.425, rewarded fraction: 0.660\n",
      "t: 74315.0, Ep: 3499.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 20.334, mean reward: 8.328, mean reward per step: 0.410, rewarded fraction: 0.556\n",
      "t: 84927.0, Ep: 3999.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 21.224, mean reward: 8.964, mean reward per step: 0.422, rewarded fraction: 0.646\n",
      "t: 95925.0, Ep: 4499.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 21.996, mean reward: 9.005, mean reward per step: 0.409, rewarded fraction: 0.600\n",
      "t: 106565.0, Ep: 4999.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 21.280, mean reward: 9.186, mean reward per step: 0.432, rewarded fraction: 0.648\n",
      "t: 116845.0, Ep: 5499.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 20.560, mean reward: 9.109, mean reward per step: 0.443, rewarded fraction: 0.616\n",
      "t: 127024.0, Ep: 5999.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 20.358, mean reward: 9.114, mean reward per step: 0.448, rewarded fraction: 0.650\n",
      "t: 137151.0, Ep: 6499.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 20.254, mean reward: 9.192, mean reward per step: 0.454, rewarded fraction: 0.672\n",
      "t: 146818.0, Ep: 6999.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 19.334, mean reward: 9.259, mean reward per step: 0.479, rewarded fraction: 0.678\n",
      "t: 156273.0, Ep: 7499.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 18.910, mean reward: 9.273, mean reward per step: 0.490, rewarded fraction: 0.680\n",
      "t: 165655.0, Ep: 7999.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 18.764, mean reward: 9.213, mean reward per step: 0.491, rewarded fraction: 0.716\n",
      "t: 175094.0, Ep: 8499.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 18.878, mean reward: 9.251, mean reward per step: 0.490, rewarded fraction: 0.682\n",
      "t: 184253.0, Ep: 8999.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 18.318, mean reward: 9.101, mean reward per step: 0.497, rewarded fraction: 0.724\n",
      "t: 193346.0, Ep: 9499.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 18.186, mean reward: 9.181, mean reward per step: 0.505, rewarded fraction: 0.674\n",
      "t: 202951.0, Ep: 9999.0, action std: 0.20, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 19.210, mean reward: 9.322, mean reward per step: 0.485, rewarded fraction: 0.694\n",
      "t: 212655.0, Ep: 10499.0, action std: 0.19, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 19.408, mean reward: 9.231, mean reward per step: 0.476, rewarded fraction: 0.698\n",
      "t: 222120.0, Ep: 10999.0, action std: 0.18, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 18.930, mean reward: 9.250, mean reward per step: 0.489, rewarded fraction: 0.700\n",
      "t: 231558.0, Ep: 11499.0, action std: 0.17, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 18.876, mean reward: 9.271, mean reward per step: 0.491, rewarded fraction: 0.670\n",
      "t: 240762.0, Ep: 11999.0, action std: 0.16, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 18.408, mean reward: 9.221, mean reward per step: 0.501, rewarded fraction: 0.718\n",
      "t: 250108.0, Ep: 12499.0, action std: 0.15, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 18.692, mean reward: 9.338, mean reward per step: 0.500, rewarded fraction: 0.688\n",
      "t: 259036.0, Ep: 12999.0, action std: 0.14, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 17.856, mean reward: 9.249, mean reward per step: 0.518, rewarded fraction: 0.678\n",
      "t: 268066.0, Ep: 13499.0, action std: 0.13, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 18.060, mean reward: 9.063, mean reward per step: 0.502, rewarded fraction: 0.684\n",
      "t: 277149.0, Ep: 13999.0, action std: 0.13, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 18.166, mean reward: 9.293, mean reward per step: 0.512, rewarded fraction: 0.710\n",
      "t: 285848.0, Ep: 14499.0, action std: 0.12, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 17.398, mean reward: 9.296, mean reward per step: 0.534, rewarded fraction: 0.680\n",
      "t: 294857.0, Ep: 14999.0, action std: 0.11, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 18.018, mean reward: 9.316, mean reward per step: 0.517, rewarded fraction: 0.664\n",
      "t: 303457.0, Ep: 15499.0, action std: 0.10, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 17.200, mean reward: 9.333, mean reward per step: 0.543, rewarded fraction: 0.688\n",
      "t: 312505.0, Ep: 15999.0, action std: 0.09, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 18.096, mean reward: 9.313, mean reward per step: 0.515, rewarded fraction: 0.652\n",
      "t: 321139.0, Ep: 16499.0, action std: 0.08, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 17.268, mean reward: 9.273, mean reward per step: 0.537, rewarded fraction: 0.700\n",
      "t: 329721.0, Ep: 16999.0, action std: 0.07, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 17.164, mean reward: 9.270, mean reward per step: 0.540, rewarded fraction: 0.694\n",
      "t: 338309.0, Ep: 17499.0, action std: 0.06, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 17.176, mean reward: 9.255, mean reward per step: 0.539, rewarded fraction: 0.702\n",
      "t: 346595.0, Ep: 17999.0, action std: 0.06, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.572, mean reward: 9.334, mean reward per step: 0.563, rewarded fraction: 0.740\n",
      "t: 355012.0, Ep: 18499.0, action std: 0.05, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.834, mean reward: 9.199, mean reward per step: 0.546, rewarded fraction: 0.698\n",
      "t: 363473.0, Ep: 18999.0, action std: 0.04, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.922, mean reward: 9.272, mean reward per step: 0.548, rewarded fraction: 0.674\n",
      "t: 371964.0, Ep: 19499.0, action std: 0.03, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.982, mean reward: 9.254, mean reward per step: 0.545, rewarded fraction: 0.708\n",
      "t: 380404.0, Ep: 19999.0, action std: 0.02, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.880, mean reward: 9.329, mean reward per step: 0.553, rewarded fraction: 0.684\n",
      "t: 389020.0, Ep: 20499.0, action std: 0.01, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 17.232, mean reward: 9.313, mean reward per step: 0.540, rewarded fraction: 0.676\n",
      "t: 397367.0, Ep: 20999.0, action std: 0.01, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.694, mean reward: 9.256, mean reward per step: 0.554, rewarded fraction: 0.748\n",
      "t: 405813.0, Ep: 21499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.892, mean reward: 9.316, mean reward per step: 0.552, rewarded fraction: 0.690\n",
      "t: 413964.0, Ep: 21999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.302, mean reward: 9.335, mean reward per step: 0.573, rewarded fraction: 0.710\n",
      "t: 422376.0, Ep: 22499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.824, mean reward: 9.266, mean reward per step: 0.551, rewarded fraction: 0.670\n",
      "t: 430687.0, Ep: 22999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.622, mean reward: 9.315, mean reward per step: 0.560, rewarded fraction: 0.702\n",
      "t: 438990.0, Ep: 23499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.606, mean reward: 9.270, mean reward per step: 0.558, rewarded fraction: 0.712\n",
      "t: 447293.0, Ep: 23999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.606, mean reward: 9.291, mean reward per step: 0.559, rewarded fraction: 0.716\n",
      "t: 455691.0, Ep: 24499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.796, mean reward: 9.278, mean reward per step: 0.552, rewarded fraction: 0.706\n",
      "t: 463984.0, Ep: 24999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.586, mean reward: 9.331, mean reward per step: 0.563, rewarded fraction: 0.744\n",
      "t: 472310.0, Ep: 25499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.652, mean reward: 9.346, mean reward per step: 0.561, rewarded fraction: 0.708\n",
      "t: 480593.0, Ep: 25999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.566, mean reward: 9.285, mean reward per step: 0.560, rewarded fraction: 0.710\n",
      "t: 488955.0, Ep: 26499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.724, mean reward: 9.286, mean reward per step: 0.555, rewarded fraction: 0.700\n",
      "t: 497077.0, Ep: 26999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.244, mean reward: 9.327, mean reward per step: 0.574, rewarded fraction: 0.704\n",
      "t: 505123.0, Ep: 27499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.092, mean reward: 9.392, mean reward per step: 0.584, rewarded fraction: 0.754\n",
      "t: 513382.0, Ep: 27999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.518, mean reward: 9.336, mean reward per step: 0.565, rewarded fraction: 0.726\n",
      "t: 521653.0, Ep: 28499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.542, mean reward: 9.311, mean reward per step: 0.563, rewarded fraction: 0.724\n",
      "t: 529709.0, Ep: 28999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.112, mean reward: 9.321, mean reward per step: 0.579, rewarded fraction: 0.760\n",
      "t: 537867.0, Ep: 29499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.316, mean reward: 9.324, mean reward per step: 0.571, rewarded fraction: 0.730\n",
      "t: 546013.0, Ep: 29999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.292, mean reward: 9.372, mean reward per step: 0.575, rewarded fraction: 0.724\n",
      "t: 554206.0, Ep: 30499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.386, mean reward: 9.333, mean reward per step: 0.570, rewarded fraction: 0.702\n",
      "t: 562423.0, Ep: 30999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.434, mean reward: 9.326, mean reward per step: 0.568, rewarded fraction: 0.686\n",
      "t: 570149.0, Ep: 31499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.452, mean reward: 9.361, mean reward per step: 0.606, rewarded fraction: 0.736\n",
      "t: 578029.0, Ep: 31999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.760, mean reward: 9.334, mean reward per step: 0.592, rewarded fraction: 0.708\n",
      "t: 586127.0, Ep: 32499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.196, mean reward: 9.309, mean reward per step: 0.575, rewarded fraction: 0.744\n",
      "t: 594261.0, Ep: 32999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.268, mean reward: 9.278, mean reward per step: 0.570, rewarded fraction: 0.744\n",
      "t: 602454.0, Ep: 33499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.386, mean reward: 9.337, mean reward per step: 0.570, rewarded fraction: 0.714\n",
      "t: 610791.0, Ep: 33999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.674, mean reward: 9.313, mean reward per step: 0.559, rewarded fraction: 0.704\n",
      "t: 619145.0, Ep: 34499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.708, mean reward: 9.297, mean reward per step: 0.556, rewarded fraction: 0.678\n",
      "t: 627273.0, Ep: 34999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.256, mean reward: 9.333, mean reward per step: 0.574, rewarded fraction: 0.708\n",
      "t: 635324.0, Ep: 35499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.102, mean reward: 9.269, mean reward per step: 0.576, rewarded fraction: 0.692\n",
      "t: 643227.0, Ep: 35999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.806, mean reward: 9.245, mean reward per step: 0.585, rewarded fraction: 0.692\n",
      "t: 651265.0, Ep: 36499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.076, mean reward: 9.302, mean reward per step: 0.579, rewarded fraction: 0.710\n",
      "t: 659479.0, Ep: 36999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.428, mean reward: 9.299, mean reward per step: 0.566, rewarded fraction: 0.730\n",
      "t: 667740.0, Ep: 37499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.522, mean reward: 9.350, mean reward per step: 0.566, rewarded fraction: 0.680\n",
      "t: 675789.0, Ep: 37999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.098, mean reward: 9.261, mean reward per step: 0.575, rewarded fraction: 0.706\n",
      "t: 683889.0, Ep: 38499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.200, mean reward: 9.358, mean reward per step: 0.578, rewarded fraction: 0.702\n",
      "t: 691995.0, Ep: 38999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.212, mean reward: 9.295, mean reward per step: 0.573, rewarded fraction: 0.712\n",
      "t: 699802.0, Ep: 39499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.614, mean reward: 9.369, mean reward per step: 0.600, rewarded fraction: 0.782\n",
      "t: 707805.0, Ep: 39999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.006, mean reward: 9.364, mean reward per step: 0.585, rewarded fraction: 0.730\n",
      "t: 715619.0, Ep: 40499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.628, mean reward: 9.339, mean reward per step: 0.598, rewarded fraction: 0.692\n",
      "t: 723664.0, Ep: 40999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.090, mean reward: 9.303, mean reward per step: 0.578, rewarded fraction: 0.740\n",
      "t: 731721.0, Ep: 41499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.114, mean reward: 9.268, mean reward per step: 0.575, rewarded fraction: 0.700\n",
      "t: 739924.0, Ep: 41999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.406, mean reward: 9.297, mean reward per step: 0.567, rewarded fraction: 0.700\n",
      "t: 747966.0, Ep: 42499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.084, mean reward: 9.276, mean reward per step: 0.577, rewarded fraction: 0.692\n",
      "t: 755726.0, Ep: 42999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.520, mean reward: 9.316, mean reward per step: 0.600, rewarded fraction: 0.686\n",
      "t: 763695.0, Ep: 43499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.938, mean reward: 9.305, mean reward per step: 0.584, rewarded fraction: 0.730\n",
      "t: 771675.0, Ep: 43999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.960, mean reward: 9.308, mean reward per step: 0.583, rewarded fraction: 0.750\n",
      "t: 779368.0, Ep: 44499.0, action std: 0.00, target radius upper bound: 0.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal trial: mean steps: 15.386, mean reward: 9.300, mean reward per step: 0.604, rewarded fraction: 0.712\n",
      "t: 787421.0, Ep: 44999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.106, mean reward: 9.258, mean reward per step: 0.575, rewarded fraction: 0.690\n",
      "t: 795585.0, Ep: 45499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.328, mean reward: 9.298, mean reward per step: 0.569, rewarded fraction: 0.690\n",
      "t: 803574.0, Ep: 45999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.978, mean reward: 9.291, mean reward per step: 0.581, rewarded fraction: 0.734\n",
      "t: 811484.0, Ep: 46499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.820, mean reward: 9.333, mean reward per step: 0.590, rewarded fraction: 0.710\n",
      "t: 819513.0, Ep: 46999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.058, mean reward: 9.314, mean reward per step: 0.580, rewarded fraction: 0.698\n",
      "t: 827675.0, Ep: 47499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.324, mean reward: 9.337, mean reward per step: 0.572, rewarded fraction: 0.666\n",
      "t: 835667.0, Ep: 47999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.984, mean reward: 9.413, mean reward per step: 0.589, rewarded fraction: 0.736\n",
      "t: 843620.0, Ep: 48499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.906, mean reward: 9.333, mean reward per step: 0.587, rewarded fraction: 0.688\n",
      "t: 851676.0, Ep: 48999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.112, mean reward: 9.345, mean reward per step: 0.580, rewarded fraction: 0.666\n",
      "t: 859680.0, Ep: 49499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.008, mean reward: 9.358, mean reward per step: 0.585, rewarded fraction: 0.712\n",
      "t: 867498.0, Ep: 49999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.636, mean reward: 9.249, mean reward per step: 0.592, rewarded fraction: 0.720\n",
      "t: 875408.0, Ep: 50499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.820, mean reward: 9.311, mean reward per step: 0.589, rewarded fraction: 0.712\n",
      "t: 883447.0, Ep: 50999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.078, mean reward: 9.343, mean reward per step: 0.581, rewarded fraction: 0.704\n",
      "t: 891247.0, Ep: 51499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.600, mean reward: 9.310, mean reward per step: 0.597, rewarded fraction: 0.752\n",
      "t: 899137.0, Ep: 51999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.780, mean reward: 9.302, mean reward per step: 0.589, rewarded fraction: 0.714\n",
      "t: 907194.0, Ep: 52499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.114, mean reward: 9.262, mean reward per step: 0.575, rewarded fraction: 0.716\n",
      "t: 915156.0, Ep: 52999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.924, mean reward: 9.089, mean reward per step: 0.571, rewarded fraction: 0.706\n",
      "t: 922807.0, Ep: 53499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.302, mean reward: 9.288, mean reward per step: 0.607, rewarded fraction: 0.746\n",
      "t: 930731.0, Ep: 53999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.848, mean reward: 9.327, mean reward per step: 0.589, rewarded fraction: 0.724\n",
      "t: 938822.0, Ep: 54499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.182, mean reward: 9.380, mean reward per step: 0.580, rewarded fraction: 0.716\n",
      "t: 947129.0, Ep: 54999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.614, mean reward: 9.331, mean reward per step: 0.562, rewarded fraction: 0.710\n",
      "t: 955179.0, Ep: 55499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.100, mean reward: 9.308, mean reward per step: 0.578, rewarded fraction: 0.720\n",
      "t: 963155.0, Ep: 55999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.952, mean reward: 9.377, mean reward per step: 0.588, rewarded fraction: 0.690\n",
      "t: 971295.0, Ep: 56499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.280, mean reward: 9.371, mean reward per step: 0.576, rewarded fraction: 0.678\n",
      "t: 979163.0, Ep: 56999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.736, mean reward: 9.389, mean reward per step: 0.597, rewarded fraction: 0.722\n",
      "t: 987164.0, Ep: 57499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.002, mean reward: 9.333, mean reward per step: 0.583, rewarded fraction: 0.702\n",
      "t: 995337.0, Ep: 57999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.346, mean reward: 9.256, mean reward per step: 0.566, rewarded fraction: 0.692\n",
      "t: 1003394.0, Ep: 58499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.114, mean reward: 9.276, mean reward per step: 0.576, rewarded fraction: 0.724\n",
      "t: 1011416.0, Ep: 58999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.044, mean reward: 9.344, mean reward per step: 0.582, rewarded fraction: 0.702\n",
      "t: 1019274.0, Ep: 59499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.716, mean reward: 9.312, mean reward per step: 0.592, rewarded fraction: 0.708\n",
      "t: 1027278.0, Ep: 59999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.008, mean reward: 9.290, mean reward per step: 0.580, rewarded fraction: 0.702\n",
      "t: 1035203.0, Ep: 60499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.850, mean reward: 9.303, mean reward per step: 0.587, rewarded fraction: 0.710\n",
      "t: 1043146.0, Ep: 60999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.886, mean reward: 9.356, mean reward per step: 0.589, rewarded fraction: 0.736\n",
      "t: 1051104.0, Ep: 61499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.916, mean reward: 9.313, mean reward per step: 0.585, rewarded fraction: 0.696\n",
      "t: 1059080.0, Ep: 61999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.952, mean reward: 9.268, mean reward per step: 0.581, rewarded fraction: 0.698\n",
      "t: 1066895.0, Ep: 62499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.630, mean reward: 9.348, mean reward per step: 0.598, rewarded fraction: 0.696\n",
      "t: 1074767.0, Ep: 62999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.744, mean reward: 9.330, mean reward per step: 0.593, rewarded fraction: 0.752\n",
      "t: 1082756.0, Ep: 63499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.978, mean reward: 9.294, mean reward per step: 0.582, rewarded fraction: 0.700\n",
      "t: 1090730.0, Ep: 63999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.948, mean reward: 9.296, mean reward per step: 0.583, rewarded fraction: 0.730\n",
      "t: 1098860.0, Ep: 64499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.260, mean reward: 9.382, mean reward per step: 0.577, rewarded fraction: 0.718\n",
      "t: 1106774.0, Ep: 64999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.828, mean reward: 9.294, mean reward per step: 0.587, rewarded fraction: 0.722\n",
      "t: 1114667.0, Ep: 65499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.786, mean reward: 9.256, mean reward per step: 0.586, rewarded fraction: 0.738\n",
      "t: 1122881.0, Ep: 65999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.428, mean reward: 9.373, mean reward per step: 0.571, rewarded fraction: 0.706\n",
      "t: 1130815.0, Ep: 66499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.868, mean reward: 9.339, mean reward per step: 0.589, rewarded fraction: 0.708\n",
      "t: 1138740.0, Ep: 66999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.850, mean reward: 9.402, mean reward per step: 0.593, rewarded fraction: 0.740\n",
      "t: 1146847.0, Ep: 67499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.214, mean reward: 9.351, mean reward per step: 0.577, rewarded fraction: 0.722\n",
      "t: 1154842.0, Ep: 67999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.990, mean reward: 9.356, mean reward per step: 0.585, rewarded fraction: 0.708\n",
      "t: 1162762.0, Ep: 68499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.840, mean reward: 9.404, mean reward per step: 0.594, rewarded fraction: 0.742\n",
      "t: 1170644.0, Ep: 68999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.764, mean reward: 9.292, mean reward per step: 0.589, rewarded fraction: 0.698\n",
      "t: 1178612.0, Ep: 69499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.936, mean reward: 9.397, mean reward per step: 0.590, rewarded fraction: 0.694\n",
      "t: 1186549.0, Ep: 69999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.874, mean reward: 9.282, mean reward per step: 0.585, rewarded fraction: 0.686\n",
      "t: 1194483.0, Ep: 70499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.868, mean reward: 9.360, mean reward per step: 0.590, rewarded fraction: 0.732\n",
      "t: 1202534.0, Ep: 70999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.102, mean reward: 9.374, mean reward per step: 0.582, rewarded fraction: 0.708\n",
      "t: 1210299.0, Ep: 71499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.530, mean reward: 9.300, mean reward per step: 0.599, rewarded fraction: 0.714\n",
      "t: 1218235.0, Ep: 71999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.872, mean reward: 9.420, mean reward per step: 0.593, rewarded fraction: 0.724\n",
      "t: 1225958.0, Ep: 72499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.446, mean reward: 9.376, mean reward per step: 0.607, rewarded fraction: 0.744\n",
      "t: 1234066.0, Ep: 72999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.216, mean reward: 9.380, mean reward per step: 0.578, rewarded fraction: 0.706\n",
      "t: 1241696.0, Ep: 73499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.260, mean reward: 9.285, mean reward per step: 0.608, rewarded fraction: 0.722\n",
      "t: 1249610.0, Ep: 73999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.828, mean reward: 9.326, mean reward per step: 0.589, rewarded fraction: 0.706\n",
      "t: 1257402.0, Ep: 74499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.584, mean reward: 9.293, mean reward per step: 0.596, rewarded fraction: 0.712\n",
      "t: 1265292.0, Ep: 74999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.780, mean reward: 9.365, mean reward per step: 0.593, rewarded fraction: 0.736\n",
      "t: 1273144.0, Ep: 75499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.704, mean reward: 9.282, mean reward per step: 0.591, rewarded fraction: 0.710\n",
      "t: 1280955.0, Ep: 75999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.622, mean reward: 9.273, mean reward per step: 0.594, rewarded fraction: 0.716\n",
      "t: 1288743.0, Ep: 76499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.576, mean reward: 9.283, mean reward per step: 0.596, rewarded fraction: 0.734\n",
      "t: 1296742.0, Ep: 76999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.998, mean reward: 9.334, mean reward per step: 0.583, rewarded fraction: 0.718\n",
      "t: 1304752.0, Ep: 77499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.020, mean reward: 9.220, mean reward per step: 0.576, rewarded fraction: 0.722\n",
      "t: 1312720.0, Ep: 77999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.936, mean reward: 9.257, mean reward per step: 0.581, rewarded fraction: 0.686\n",
      "t: 1320560.0, Ep: 78499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.680, mean reward: 9.355, mean reward per step: 0.597, rewarded fraction: 0.734\n",
      "t: 1328617.0, Ep: 78999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.114, mean reward: 9.316, mean reward per step: 0.578, rewarded fraction: 0.684\n",
      "t: 1336684.0, Ep: 79499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.134, mean reward: 9.333, mean reward per step: 0.578, rewarded fraction: 0.690\n",
      "t: 1344472.0, Ep: 79999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.576, mean reward: 9.331, mean reward per step: 0.599, rewarded fraction: 0.714\n",
      "t: 1352356.0, Ep: 80499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.768, mean reward: 9.296, mean reward per step: 0.590, rewarded fraction: 0.700\n",
      "t: 1360315.0, Ep: 80999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.918, mean reward: 9.286, mean reward per step: 0.583, rewarded fraction: 0.720\n",
      "t: 1368227.0, Ep: 81499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.824, mean reward: 9.334, mean reward per step: 0.590, rewarded fraction: 0.738\n",
      "t: 1376057.0, Ep: 81999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.660, mean reward: 9.369, mean reward per step: 0.598, rewarded fraction: 0.738\n",
      "t: 1384060.0, Ep: 82499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.006, mean reward: 9.300, mean reward per step: 0.581, rewarded fraction: 0.700\n",
      "t: 1391941.0, Ep: 82999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.762, mean reward: 9.337, mean reward per step: 0.592, rewarded fraction: 0.760\n",
      "t: 1399777.0, Ep: 83499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.672, mean reward: 9.304, mean reward per step: 0.594, rewarded fraction: 0.714\n",
      "t: 1407853.0, Ep: 83999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.152, mean reward: 9.353, mean reward per step: 0.579, rewarded fraction: 0.682\n",
      "t: 1415719.0, Ep: 84499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.732, mean reward: 9.312, mean reward per step: 0.592, rewarded fraction: 0.718\n",
      "t: 1423729.0, Ep: 84999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.020, mean reward: 9.358, mean reward per step: 0.584, rewarded fraction: 0.728\n",
      "t: 1431607.0, Ep: 85499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.756, mean reward: 9.359, mean reward per step: 0.594, rewarded fraction: 0.716\n",
      "t: 1439595.0, Ep: 85999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.976, mean reward: 9.262, mean reward per step: 0.580, rewarded fraction: 0.698\n",
      "t: 1447681.0, Ep: 86499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.172, mean reward: 9.272, mean reward per step: 0.573, rewarded fraction: 0.702\n",
      "t: 1455862.0, Ep: 86999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.362, mean reward: 9.227, mean reward per step: 0.564, rewarded fraction: 0.682\n",
      "t: 1463961.0, Ep: 87499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.198, mean reward: 9.361, mean reward per step: 0.578, rewarded fraction: 0.652\n",
      "t: 1471767.0, Ep: 87999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.612, mean reward: 9.346, mean reward per step: 0.599, rewarded fraction: 0.718\n",
      "t: 1479846.0, Ep: 88499.0, action std: 0.00, target radius upper bound: 0.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal trial: mean steps: 16.158, mean reward: 9.282, mean reward per step: 0.574, rewarded fraction: 0.686\n",
      "t: 1487729.0, Ep: 88999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.766, mean reward: 9.263, mean reward per step: 0.588, rewarded fraction: 0.700\n",
      "t: 1495657.0, Ep: 89499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.856, mean reward: 9.282, mean reward per step: 0.585, rewarded fraction: 0.704\n",
      "t: 1503507.0, Ep: 89999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.700, mean reward: 9.230, mean reward per step: 0.588, rewarded fraction: 0.718\n",
      "t: 1511440.0, Ep: 90499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.866, mean reward: 9.342, mean reward per step: 0.589, rewarded fraction: 0.732\n",
      "t: 1519356.0, Ep: 90999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.832, mean reward: 9.310, mean reward per step: 0.588, rewarded fraction: 0.734\n",
      "t: 1527271.0, Ep: 91499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.830, mean reward: 9.306, mean reward per step: 0.588, rewarded fraction: 0.686\n",
      "t: 1535218.0, Ep: 91999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.894, mean reward: 9.366, mean reward per step: 0.589, rewarded fraction: 0.702\n",
      "t: 1543103.0, Ep: 92499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.770, mean reward: 9.276, mean reward per step: 0.588, rewarded fraction: 0.716\n",
      "t: 1550953.0, Ep: 92999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.700, mean reward: 9.372, mean reward per step: 0.597, rewarded fraction: 0.704\n",
      "t: 1558856.0, Ep: 93499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.806, mean reward: 9.312, mean reward per step: 0.589, rewarded fraction: 0.756\n",
      "t: 1566753.0, Ep: 93999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.794, mean reward: 9.333, mean reward per step: 0.591, rewarded fraction: 0.726\n",
      "t: 1574451.0, Ep: 94499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.396, mean reward: 9.289, mean reward per step: 0.603, rewarded fraction: 0.708\n",
      "t: 1582451.0, Ep: 94999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.000, mean reward: 9.404, mean reward per step: 0.588, rewarded fraction: 0.722\n",
      "t: 1590438.0, Ep: 95499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.974, mean reward: 9.264, mean reward per step: 0.580, rewarded fraction: 0.726\n",
      "t: 1598387.0, Ep: 95999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.898, mean reward: 9.358, mean reward per step: 0.589, rewarded fraction: 0.670\n",
      "t: 1606324.0, Ep: 96499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.874, mean reward: 9.340, mean reward per step: 0.588, rewarded fraction: 0.718\n",
      "t: 1614265.0, Ep: 96999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.882, mean reward: 9.309, mean reward per step: 0.586, rewarded fraction: 0.692\n",
      "t: 1622346.0, Ep: 97499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.162, mean reward: 9.124, mean reward per step: 0.565, rewarded fraction: 0.692\n",
      "t: 1630589.0, Ep: 97999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.486, mean reward: 9.187, mean reward per step: 0.557, rewarded fraction: 0.694\n",
      "t: 1638811.0, Ep: 98499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.444, mean reward: 9.268, mean reward per step: 0.564, rewarded fraction: 0.704\n",
      "t: 1646717.0, Ep: 98999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.812, mean reward: 9.322, mean reward per step: 0.590, rewarded fraction: 0.686\n",
      "t: 1654511.0, Ep: 99499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.588, mean reward: 9.288, mean reward per step: 0.596, rewarded fraction: 0.736\n",
      "t: 1662661.0, Ep: 99999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.300, mean reward: 9.360, mean reward per step: 0.574, rewarded fraction: 0.698\n",
      "t: 1670518.0, Ep: 100499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.714, mean reward: 9.337, mean reward per step: 0.594, rewarded fraction: 0.708\n",
      "t: 1678428.0, Ep: 100999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.820, mean reward: 9.321, mean reward per step: 0.589, rewarded fraction: 0.716\n",
      "t: 1686273.0, Ep: 101499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.690, mean reward: 9.339, mean reward per step: 0.595, rewarded fraction: 0.752\n",
      "t: 1694341.0, Ep: 101999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.136, mean reward: 9.367, mean reward per step: 0.581, rewarded fraction: 0.712\n",
      "t: 1702134.0, Ep: 102499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.586, mean reward: 9.385, mean reward per step: 0.602, rewarded fraction: 0.734\n",
      "t: 1710243.0, Ep: 102999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.218, mean reward: 9.289, mean reward per step: 0.573, rewarded fraction: 0.694\n",
      "t: 1718506.0, Ep: 103499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.526, mean reward: 9.424, mean reward per step: 0.570, rewarded fraction: 0.724\n",
      "t: 1726394.0, Ep: 103999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.776, mean reward: 9.270, mean reward per step: 0.588, rewarded fraction: 0.702\n",
      "t: 1734458.0, Ep: 104499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.128, mean reward: 9.325, mean reward per step: 0.578, rewarded fraction: 0.708\n",
      "t: 1742631.0, Ep: 104999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.346, mean reward: 9.293, mean reward per step: 0.569, rewarded fraction: 0.722\n",
      "t: 1750714.0, Ep: 105499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.166, mean reward: 9.364, mean reward per step: 0.579, rewarded fraction: 0.698\n",
      "t: 1758876.0, Ep: 105999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.324, mean reward: 9.246, mean reward per step: 0.566, rewarded fraction: 0.686\n",
      "t: 1766865.0, Ep: 106499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 15.978, mean reward: 9.370, mean reward per step: 0.586, rewarded fraction: 0.750\n",
      "t: 1774871.0, Ep: 106999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.012, mean reward: 9.339, mean reward per step: 0.583, rewarded fraction: 0.722\n",
      "t: 1783081.0, Ep: 107499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.420, mean reward: 9.270, mean reward per step: 0.565, rewarded fraction: 0.694\n",
      "t: 1791190.0, Ep: 107999.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.218, mean reward: 9.321, mean reward per step: 0.575, rewarded fraction: 0.722\n",
      "t: 1799669.0, Ep: 108499.0, action std: 0.00, target radius upper bound: 0.250\n",
      "Normal trial: mean steps: 16.958, mean reward: 9.168, mean reward per step: 0.541, rewarded fraction: 0.732\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-567521ed13ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mnext_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreached_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperturbation_v_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperturbation_w_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#track true next_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m         \u001b[0mnext_b_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_b_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_orgin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperturbation_v_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperturbation_w_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBstep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_b_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_b_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# reshaped updated belief\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-9da6c11068b7>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, b_v, b_x, x, a, a_orgin, perturbation_v_t, perturbation_w_t)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mb_x_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mb_x_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb_x_t_last\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb_v_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheading_angle\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mb_x_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb_x_t_last\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb_v_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheading_angle\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mb_x_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheading_angle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mA_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tot_t = 0. # number of total time steps\n",
    "episode = 0.\n",
    "episode_number_perturbation = 0\n",
    "reward_log_perturbation = 0\n",
    "rewarded_number_log_perturbation = 0\n",
    "step_log_perturbation = 0\n",
    "episode_number_normal = 0\n",
    "reward_log_normal = 0\n",
    "rewarded_number_log_normal = 0\n",
    "step_log_normal = 0\n",
    "policy_loss_log = 0\n",
    "value_loss_log = 0\n",
    "\n",
    "while tot_t<arg.TOT_T: #arg.TOT_T\n",
    "    t = torch.zeros(1) # time step in one trial\n",
    "    \n",
    "    cross_start_threshold = False\n",
    "    done = torch.tensor([0]) # done flag\n",
    "    while t <= arg.EPISODE_LEN: # 70 steps==7s\n",
    "        action, action_orgin = agent.select_action(state, action_noise = noise)\n",
    "        \n",
    "        if not cross_start_threshold and not is_terminal_action(action_orgin, arg.TERMINAL_ACTION): # start threshold\n",
    "            cross_start_threshold = True\n",
    "            \n",
    "        next_x, reached_target, perturbation_v_t, perturbation_w_t = env(x, action, t) #track true next_x\n",
    "        next_b_v, next_b_x, info = agent.Bstep(b_v, b_x, next_x, action, action_orgin, perturbation_v_t, perturbation_w_t)\n",
    "        t += 1\n",
    "        next_state = agent.Bstep.Breshape(next_b_v, next_b_x, t) # reshaped updated belief\n",
    "        \n",
    "        reward = return_reward(target_position, info, reached_target, next_b_x, env.goal_radius, \n",
    "                               arg.REWARD, finetuning=0) # gaussian reward based on belief\n",
    "        \n",
    "        TimeEnd = (t == arg.EPISODE_LEN) # if the monkey can't catch the firefly in EPISODE_LEN, reset the game.\n",
    "        \n",
    "        if (info['stop'] and cross_start_threshold) or TimeEnd:\n",
    "            isPerturbation_log = isPerturbation\n",
    "            done = torch.tensor([1])\n",
    "            next_x, target_position, isPerturbation, pro_gains, pro_noise_covariance, goal_radius = env.reset()\n",
    "            \n",
    "            next_b_v, next_b_x, next_state = agent.Bstep.reset(pro_gains, pro_noise_covariance, goal_radius,\n",
    "                                                   target_position, isPerturbation)\n",
    "            \n",
    "        agent.memory.push(state.cuda(), action, next_state.cuda(), reward.cuda(), done.cuda()) #episodic\n",
    "        #agent.memory.push(state.cuda(), action, next_state.cuda(), reward.cuda(), torch.tensor([0]).cuda()) # continous\n",
    "        \n",
    "        if tot_t > 1000:\n",
    "            policy_loss, value_loss = agent.learn(arg.BATCH_SIZE, tot_t) # sample from buffer,back-propagate\n",
    "            policy_loss_log += policy_loss\n",
    "            value_loss_log += value_loss\n",
    "        \n",
    "        # update variables\n",
    "        x = next_x\n",
    "        state = next_state\n",
    "        b_v = next_b_v \n",
    "        b_x = next_b_x\n",
    "        tot_t += 1\n",
    "        \n",
    "        if episode > 1e4 and tot_t % 100 == 0:\n",
    "            # update action space exploration noise\n",
    "            action_std -= arg.STD_STEP_SIZE  # exploration noise\n",
    "            action_std = torch.max(torch.tensor(0.001), action_std)\n",
    "            noise.reset(torch.tensor(0), action_std)\n",
    "                \n",
    "        if (info['stop'] and cross_start_threshold) or TimeEnd:\n",
    "            break\n",
    "       \n",
    "    # end of one episode, do some checking\n",
    "    episode += 1 # trial or episode number\n",
    "    if isPerturbation_log:\n",
    "        episode_number_perturbation += 1\n",
    "        reward_log_perturbation += reward.item()\n",
    "        rewarded_number_log_perturbation += int(reached_target)\n",
    "        step_log_perturbation += t.item()\n",
    "    else: \n",
    "        episode_number_normal += 1\n",
    "        reward_log_normal += reward.item()\n",
    "        rewarded_number_log_normal += int(reached_target)\n",
    "        step_log_normal += t.item()\n",
    "        \n",
    "    if episode % 500 == 499:  \n",
    "        agent.save()\n",
    "        \n",
    "        print (\"t: {}, Ep: {}, action std: {:0.2f}, target radius upper bound: {:0.3f}\".format(tot_t,episode,noise.scale,\n",
    "                                                                                    env.initial_radius_upper_bound))\n",
    "        print (\"Normal trial: mean steps: {:0.3f}, mean reward: {:0.3f}, mean reward per step: {:0.3f}, \\\n",
    "rewarded fraction: {:0.3f}\".format(\n",
    "        step_log_normal/episode_number_normal, reward_log_normal/episode_number_normal, reward_log_normal/step_log_normal,\n",
    "        rewarded_number_log_normal/episode_number_normal))\n",
    "        #print (\"Perturbation trial: mean steps: {:0.3f}, mean reward: {:0.3f}, mean reward per step: {:0.3f}, \\\n",
    "#rewarded fraction: {:0.3f}\\n\".format(\n",
    "        #step_log_perturbation/episode_number_perturbation, reward_log_perturbation/episode_number_perturbation, \n",
    "            #reward_log_perturbation/step_log_perturbation,rewarded_number_log_perturbation/episode_number_perturbation))\n",
    "        \n",
    "        \n",
    "        writer.add_scalar('average policy loss',policy_loss_log*2/(step_log_perturbation+step_log_normal),episode)\n",
    "        writer.add_scalar('average value loss',value_loss_log/(step_log_perturbation+step_log_normal),episode)\n",
    "        \n",
    "        writer.add_scalar('average steps per normal trial',step_log_normal/episode_number_normal,episode)\n",
    "        writer.add_scalar('average reward per normal trial',reward_log_normal/episode_number_normal,episode)\n",
    "        writer.add_scalar('average reward per normal step',reward_log_normal/step_log_normal,episode)\n",
    "        writer.add_scalar('average rewarded ratio per normal trial',\n",
    "                          rewarded_number_log_normal/episode_number_normal,episode)\n",
    "        \n",
    "        #writer.add_scalar('average steps per perturbation trial',step_log_perturbation/episode_number_perturbation,episode)\n",
    "        #writer.add_scalar('average reward per perturbation trial',\n",
    "        #                  reward_log_perturbation/episode_number_perturbation,episode)\n",
    "        #writer.add_scalar('average reward per perturbation step',reward_log_perturbation/step_log_perturbation,episode)\n",
    "        #writer.add_scalar('average rewarded ratio per perturbation trial',\n",
    "        #                  rewarded_number_log_perturbation/episode_number_perturbation,episode)\n",
    "        \n",
    "        episode_number_perturbation = 0\n",
    "        reward_log_perturbation = 0\n",
    "        rewarded_number_log_perturbation = 0\n",
    "        step_log_perturbation = 0\n",
    "        episode_number_normal = 0\n",
    "        reward_log_normal = 0\n",
    "        rewarded_number_log_normal = 0\n",
    "        step_log_normal = 0\n",
    "        policy_loss_log = 0\n",
    "        value_loss_log = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
