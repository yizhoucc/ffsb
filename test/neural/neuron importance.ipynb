{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantify neuron importance in nonlinear decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the general question is, how shoudl we quanitfy the importance of each feature dim.\n",
    "\n",
    "some thoughts:\n",
    "- Permutation: permuting each feature and measuring the drop in model performance to determine the importance of each feature. its like the inference dropout. if the feature is more important, the acc should drop more.\n",
    "- Feature Sensitivity Analysis: By systematically varying one feature while keeping others fixed and observing the effect on model output, you can gauge the sensitivity of the model to each feature. eg, we when the feaure vary from 0-1, how much would the output vary. the large the output varys, the more important the feature.\n",
    "- Partial Derivatives: Analyzing the partial derivatives of the output with respect to each input can provide insights into the impact of individual dimensions on the output. the idea is simlar to above. if the partial diff is larger, it means if we chagne the dim slightly, the output will change greatly.\n",
    "- PCA: PCA can be used to identify the dimensions that capture the most variance in the data. Dimensions with higher variance are typically considered more important. \n",
    "but it ignores that some small variation that truly correlate to the output.\n",
    "- Information Gain: Information theoretic measures such as entropy or mutual information can be used to quantify the amount of information provided by each dimension. not sure how to start.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ground truth\n",
    "we make a ground truth to test this.\n",
    "x has 3 dim. dim1 is truely correlate to y with some noise, dim2 has larger noise and correlate to y less, dim3 is not in correlation to y at all.\n",
    "y is 1 dim.\n",
    "then we train a tiny torch network to predict y from x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Generate some synthetic data (you'll replace this with your actual data)\n",
    "# Assume x has shape (num_samples, 3) and y has shape (num_samples, 1)\n",
    "# You'll need to replace these with your actual data\n",
    "x = torch.rand((100, 3))  # Example: 100 samples, 3 dimensions\n",
    "y = 2 * x[:, 0] + 0.5 * x[:, 1] - 0.3 * x[:, 2] + torch.randn((100, 1)) * 0.1\n",
    "\n",
    "# Define a simple neural network\n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 10)  # Fully connected layer 1\n",
    "        self.fc2 = nn.Linear(10, 1)  # Fully connected layer 2 (output layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = TinyNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted y for new input: -0.0594\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Generate some synthetic data (you'll replace this with your actual data)\n",
    "# Assume x has shape (num_samples, 3) and y has shape (num_samples, 1)\n",
    "# You'll need to replace these with your actual data\n",
    "x = torch.rand((100, 3))  # Example: 100 samples, 3 dimensions\n",
    "y = 2 * x[:, 0] + 0.5 * x[:, 1] - 0.3 * x[:, 2] + torch.randn((100, 1)) * 0.1\n",
    "\n",
    "# Define a simple neural network\n",
    "class TinyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TinyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 10)  # Fully connected layer 1\n",
    "        self.fc2 = nn.Linear(10, 1)  # Fully connected layer 2 (output layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = TinyNet()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean squared error loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Stochastic gradient descent\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):  # You can adjust the number of epochs\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(x)\n",
    "    loss = criterion(outputs, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/1000], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Now you can use the trained model to predict y for new input data\n",
    "new_x = torch.tensor([[0.8, 0.2, 0.5]])  # Example new input\n",
    "predicted_y = model(new_x)\n",
    "print(f\"Predicted y for new input: {predicted_y.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: feature sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### sum of weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### partial derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vary 0-1 and fix other dim to 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lab)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
