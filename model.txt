phi, theta = parameter set( 
    11 parameter:
    control gain foward. scalar
    control gain angular
    contorl noise forward, as std
    contorl noise angular
    obs gain forward, in H, to transform state space to obs space(velocity only)
    obs gain angular
    obs noise forward
    obs noise angular
    acceleration time decay tau, vt+1=a*vt+b*(ut+ control noise)*control gain)
    magnitude action cost, cost_t=this parameter scalar*||contorl_t||
    deverivative action cost==this parameter scalar*(||contorl_t||-||contorl_t-1||)

)

loglikelihood(of policy(observationS,inital_state, theta)=policy(actionS|observationS,inital_state, estimation of theta))
'trained agent produce same control trajectory, given same inital_state, obs trajectory, but with theta/estimation of theta'

logll=logp(actions|observationS, inital_state, theta)=
    logp(observations|states,theta) because observation is from state
        obs=H@state+noise
    logp(states|actions, inital_state, theta) because action changes stats, 
        states=A(action)@state_t-1+B@action+noise
    logp(inital_state, theta) ignored because random initilization
        can be ingoreed.
