{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "from numpy import pi\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define actor and critic networks\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        num_outputs = action_dim\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.mu = nn.Linear(hidden_dim, num_outputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        :param inputs: state = torch.cat([r, rel_ang, vel, ang_vel, time, vecL])\n",
    "        inputs[:,1]: rel_ang\n",
    "        inputs[:,3]: ang_vel\n",
    "        \"\"\"\n",
    "        x = self.linear1(inputs)\n",
    "        x = self.relu1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "        mu = torch.tanh(self.mu(x))\n",
    "        return mu\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim, hidden_dim=128):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.linear_action = nn.Linear(action_dim, hidden_dim)\n",
    "        self.relu1_action = nn.ReLU(inplace=True)\n",
    "        self.linear2 = nn.Linear(hidden_dim + hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.V = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, inputs, actions):\n",
    "\n",
    "        x_input = self.linear1(inputs)\n",
    "        x_input = self.relu1(x_input)\n",
    "        x_action = self.linear_action(actions)\n",
    "        x_action = self.relu1_action(x_action)\n",
    "        x = torch.cat((x_input, x_action), dim=1)\n",
    "        x = self.linear2(x)\n",
    "        x = self.relu2(x)\n",
    "\n",
    "        V = self.V(x)\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent utils\n",
    "\n",
    "def soft_update(target, source, tau):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(target_param.data * (1.0 - tau) + param.data * tau)\n",
    "\n",
    "def hard_update(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "\n",
    "def next_path(path_pattern):\n",
    "    \"\"\"\n",
    "    path_pattern = 'file-%s.txt':\n",
    "    \"\"\"\n",
    "    i = 1\n",
    "    while os.path.exists(path_pattern % i):\n",
    "        i = i * 2\n",
    "\n",
    "    a, b = (i // 2, i)\n",
    "    while a + 1 < b:\n",
    "        c = (a + b) // 2 # interval midpoint\n",
    "        a, b = (c, b) if os.path.exists(path_pattern % c) else (a, c)\n",
    "\n",
    "    return path_pattern % b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SumTree and PER\n",
    "# a binary tree data structure where the parent’s value is the sum of its children\n",
    "class SumTree(object):\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        self.n_entries = 0\n",
    "\n",
    "    # update to the root node\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # find sample on leaf node\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "\n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    # store priority and sample\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    # update priority\n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "\n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "\n",
    "    # get priority and sample\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_entries\n",
    "\n",
    "\n",
    "# PER\n",
    "class PER(object):  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.0001\n",
    "    #a = 0.6\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "\n",
    "    def __init__(self, capacity, a=1.):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "        self.a = a\n",
    "\n",
    "    def _get_priority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / n\n",
    "        priorities = []\n",
    "\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'done', 'next_state', 'reward'))\n",
    "    #'Transition', ('state', 'action', 'mask', 'next_state', 'reward'))\n",
    "    \n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity, priority=False):\n",
    "        self.capacity = capacity\n",
    "        self.priority = priority\n",
    "        if priority:\n",
    "            self.memory = PER(capacity=capacity)\n",
    "        else:\n",
    "            self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, *args, err=None):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if self.priority:\n",
    "            assert err is not None, \"Need to pass float error to add to priority memory\"\n",
    "            self.memory.add(err, Transition(*args))\n",
    "        else:\n",
    "            self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if self.priority:\n",
    "            batch, idx, is_weights = self.memory.sample(batch_size)\n",
    "        else:\n",
    "            batch = random.sample(self.memory, batch_size)\n",
    "            idx = None\n",
    "        batch = Transition(*zip(*batch))\n",
    "        return batch, idx\n",
    "\n",
    "    def update(self, idx, err):\n",
    "        assert self.priority, \"Cannot call this function if not priority memory\"\n",
    "        self.memory.update(idx, err)\n",
    "\n",
    "    def batch_update(self, ids, errs):\n",
    "        for idx, err in zip(ids, errs):\n",
    "            self.update(idx, err)\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards\n",
    "\"\"\"\n",
    "reward.py\n",
    "This file describes reward function which is the “expected reward” for the belief distribution over Gaussian reward distribution.\n",
    "rew_std: standard deviation of Gaussian distribution for reward [std_x, std_y]\n",
    "\n",
    "b(s)= 1/sqrt(2*pi*det(P)) * exp(-0.5* ((s-x)^T*P^-1*(s-x)) : Gaussian distribution with mean x, covariance P\n",
    "r(s) = scale * exp(-0.5* s^T* R^-1 * s): reward gaussian distribution with mean zeros, covariance R\n",
    "invS = invR +invP\n",
    "R(b) = \\int b(s)*r(s) ds = c *sqrt(det(S)/det(P))* exp(-0.5* mu^T*(invP - invP*S*invP)*mu)\n",
    "\n",
    "R(b) =  \\int b(s)*r(s) ds = 1/sqrt(det(2 pi (P+R)) * exp(-0.5*mu^t(R+P)^-1*mu)\n",
    "\"\"\"\n",
    "\n",
    "def return_reward(episode, info, reached_target, b, goal_radius, REWARD, finetuning = 0):\n",
    "    if info['stop']:  # receive reward if monkey stops. position does not matters\n",
    "        \n",
    "        if finetuning == 0: # Gaussian reward based on belief\n",
    "            reward = get_reward(b, goal_radius, REWARD)\n",
    "            \n",
    "        else: # 0/1 reward based on real position\n",
    "            if reached_target == 1:\n",
    "                reward = REWARD * torch.ones(1).cuda()\n",
    "            else:\n",
    "                reward = -0 * torch.ones(1).cuda()\n",
    "    else:\n",
    "        reward = -0 * torch.ones(1).cuda()\n",
    "    return reward\n",
    "\n",
    "\n",
    "def get_reward(b, goal_radius, REWARD):\n",
    "    bx, P = b\n",
    "    rew_std = goal_radius / 2  # std of reward function --> 2*std (=goal radius) = reward distribution\n",
    "\n",
    "    #rew_std = goal_radus/2/2 #std of reward function --> 2*std (=goal radius) = reward distribution\n",
    "    reward = rewardFunc(rew_std, bx.view(-1), P, REWARD)  # reward currently only depends on belief not action\n",
    "    return reward\n",
    "\n",
    "\"\"\"\n",
    "def rewardFunc(rew_std, x, P, scale):\n",
    "    R = torch.eye(2) * rew_std**2 # reward function is gaussian\n",
    "    P = P[:2, :2] # cov\n",
    "    invP = torch.inverse(P)\n",
    "    invS = torch.inverse(R) + invP\n",
    "    S = torch.inverse(invS)\n",
    "    mu = x[:2] # pos\n",
    "    alpha = -0.5 * mu.matmul(invP - invP.mm(S).mm(invP)).matmul(mu)\n",
    "    reward = torch.exp(alpha) * torch.sqrt(torch.det(S)/torch.det(P))\n",
    "    reward = scale * reward # adjustment for reward per timestep\n",
    "    return reward.view(-1)\n",
    "\"\"\"\n",
    "\n",
    "def rewardFunc(rew_std, x, P, scale):\n",
    "    mu = x[:2]  # pos\n",
    "    R = torch.eye(2).cuda() * rew_std**2 # reward function is gaussian\n",
    "    P = P[:2, :2] # cov\n",
    "    S = R+P\n",
    "    if not is_pos_def(S):\n",
    "        print('R+P is not positive definite!')\n",
    "    alpha = -0.5 * mu @ S.inverse() @ mu.t()\n",
    "    #alpha = -0.5 * mu.matmul(torch.inverse(R+P)).matmul(mu.t())\n",
    "    reward = torch.exp(alpha) /2 / np.pi /torch.sqrt(S.det())\n",
    "\n",
    "    # normalization -> to make max reward as 1\n",
    "    mu_zero = torch.zeros(1,2).cuda()\n",
    "    alpha_zero = -0.5 * mu_zero @ R.inverse() @ mu_zero.t()\n",
    "    reward_zero = torch.exp(alpha_zero) /2 / np.pi /torch.sqrt(R.det())\n",
    "    reward = reward/reward_zero\n",
    "    ####################\n",
    "\n",
    "    reward = scale * reward  # adjustment for reward per timestep\n",
    "    if reward > scale:\n",
    "        print('reward is wrong!', reward)\n",
    "        print('mu', mu)\n",
    "        print('P', P)\n",
    "        print('R', R)\n",
    "    return reward.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#terminal\n",
    "def is_terminal_action(a, terminal_vel):\n",
    "    \"\"\"\n",
    "    terminal is true if the action( which determines velocity) is lower that terminal_vel,\n",
    "    which means the monkey stops.\n",
    "    This approach only cares the action, does not depend on the position.\n",
    "    \"\"\"\n",
    "    stop = (torch.norm(a) < terminal_vel)\n",
    "\n",
    "    if stop:\n",
    "        return torch.ByteTensor([True])\n",
    "    else:\n",
    "        return torch.ByteTensor([False])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env utils\n",
    "def is_pos_def(x):\n",
    "    \"\"\"\n",
    "    Check if the matrix is positive definite\n",
    "    \"\"\"\n",
    "    x = x.detach().cpu().numpy()\n",
    "    return np.all(np.linalg.eigvalsh(x) > 0)\n",
    "\n",
    "def tril_mask(size):\n",
    "    \"\"\"\n",
    "    Returns a lower triangular mask\n",
    "    (Used to select lower triangular elements)\n",
    "    \"\"\"\n",
    "    mask = torch.tril(torch.ones(size, size, dtype=torch.uint8)).cuda() # ByteTensor\n",
    "    return mask\n",
    "\n",
    "def vectorLowerCholesky(P):\n",
    "    \"\"\"\n",
    "    Performs the lower cholesky decomposition and returns vectorized output\n",
    "    P = L L.t()\n",
    "    \"\"\"\n",
    "    L = torch.cholesky(P, upper=False)\n",
    "    mask = tril_mask(P.size(0))\n",
    "    return torch.masked_select(L, mask > 0)\n",
    "\n",
    "def sample_exp(min, max, scale = np.e):\n",
    "    \"\"\"sample a random number with exponetial distribution\n",
    "    the number should be in a range [min, max]\n",
    "    should be min, max >= 0\n",
    "    \"\"\"\n",
    "    temp = min -100\n",
    "    while temp < min or temp > max:\n",
    "        temp = np.random.exponential(scale=scale)\n",
    "    return temp\n",
    "\n",
    "def range_angle(ang):\n",
    "    \"\"\"\n",
    "    Adjusts the range of angle from -pi to pi\n",
    "    \"\"\"\n",
    "    ang = torch.remainder(ang, 2*pi)\n",
    "    ang = ang if ang < pi else (ang -2*pi)\n",
    "    return ang\n",
    "\n",
    "\n",
    "def dynamics(x, a, dt, box, pro_gains, pro_noise_ln_vars):\n",
    "    # dynamics\n",
    "    px, py, ang, vel, ang_vel = torch.split(x.view(-1), 1)\n",
    "\n",
    "    a_v = a[0]  # action for velocity\n",
    "    a_w = a[1]  # action for angular velocity\n",
    "\n",
    "    w = torch.sqrt(torch.exp(pro_noise_ln_vars)) * torch.randn(2).cuda() # std * randn #random process noise for [vel, ang_vel]\n",
    "\n",
    "    vel = torch.tensor(0.0).cuda() * vel + pro_gains[0] * a_v + w[0]\n",
    "    ang_vel = torch.tensor(0.0).cuda() * ang_vel + pro_gains[1] * a_w + w[1]\n",
    "    ang = ang + ang_vel * dt\n",
    "    ang = range_angle(ang) # adjusts the range of angle from -pi to pi\n",
    "\n",
    "    px = px + vel * torch.cos(ang) * dt\n",
    "    py = py + vel * torch.sin(ang) * dt\n",
    "    px = torch.clamp(px, -box, box)\n",
    "    py = torch.clamp(py, -box, box)\n",
    "    next_x = torch.stack((px, py, ang, vel, ang_vel))\n",
    "\n",
    "    return next_x.view(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "belief_step.py\n",
    "This code uses the polar coordinate\n",
    "state = torch.cat([vel, ang_vel, r, ang, vecL, time])\n",
    "\"\"\"\n",
    "\n",
    "class BeliefStep(nn.Module):\n",
    "    def __init__(self, arg):\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.dt = arg.DELTA_T\n",
    "        self.P = torch.eye(5).cuda() * 1e-8\n",
    "        self.terminal_vel = arg.TERMINAL_VEL\n",
    "        return\n",
    "\n",
    "    def reset(self, x, time, pro_gains, pro_noise_ln_vars, goal_radius, gains_range, noise_range, obs_gains = None, \n",
    "              obs_noise_ln_vars = None):\n",
    "\n",
    "        self.pro_gains = pro_gains\n",
    "        self.pro_noise_ln_vars = pro_noise_ln_vars\n",
    "        self.goal_radius = goal_radius\n",
    "\n",
    "        self.obs_gains = torch.zeros(2).cuda()\n",
    "        self.obs_noise_ln_vars = torch.zeros(2).cuda()\n",
    "\n",
    "\n",
    "        if obs_gains is None:\n",
    "            self.obs_gains[0] = torch.zeros(1).uniform_(gains_range[0], gains_range[1])  # [obs_gain_vel]\n",
    "            self.obs_gains[1] = torch.zeros(1).uniform_(gains_range[2], gains_range[3])  # [obs_gain_ang]\n",
    "        else:\n",
    "            self.obs_gains = obs_gains\n",
    "\n",
    "        if obs_noise_ln_vars is None:\n",
    "            self.obs_noise_ln_vars[0] = -1 * sample_exp(-noise_range[1], -noise_range[0]) # [obs_vel_noise]\n",
    "            self.obs_noise_ln_vars[1] = -1 * sample_exp(-noise_range[3], -noise_range[2]) # [obs_ang_noise]\n",
    "        else:\n",
    "            self.obs_noise_ln_vars = obs_noise_ln_vars\n",
    "\n",
    "        \"\"\"    \n",
    "        if obs_noise_stds is None:\n",
    "            self.obs_noise_stds[0] = torch.zeros(1).uniform_(std_range[0], std_range[1])  # [obs_vel_noise]\n",
    "            self.obs_noise_stds[1] = torch.zeros(1).uniform_(std_range[2], std_range[3])  # [obs_ang_noise]\n",
    "        else:\n",
    "            self.obs_noise_stds = obs_noise_stds\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        self.theta = (self.pro_gains, self.pro_noise_ln_vars, self.obs_gains, self.obs_noise_ln_vars, self.goal_radius)\n",
    "\n",
    "\n",
    "        self.P = torch.eye(5).cuda() * 1e-8 # change 4 to size function\n",
    "        self.b = x, self.P  # belief\n",
    "        self.state = self.Breshape(self.b, time, self.theta)\n",
    "\n",
    "        return self.b, self.state, self.obs_gains, self.obs_noise_ln_vars\n",
    "\n",
    "    def forward(self,  b, ox, a, box):\n",
    "        I = torch.eye(5).cuda()\n",
    "\n",
    "        # Q matrix\n",
    "        Q = torch.zeros(5, 5).cuda()\n",
    "        Q[-2:, -2:] = torch.diag(torch.exp(self.pro_noise_ln_vars)) # variance of vel, ang_vel\n",
    "\n",
    "        # R matrix\n",
    "        R = torch.diag(torch.exp(self.obs_noise_ln_vars))\n",
    "\n",
    "\n",
    "        # H matrix\n",
    "        H = torch.zeros(2, 5).cuda()\n",
    "        H[:, -2:] = torch.diag(self.obs_gains)\n",
    "\n",
    "\n",
    "        # Extended Kalman Filter\n",
    "        pre_bx_, P = b\n",
    "        bx_ = dynamics(pre_bx_, a.view(-1), self.dt, box, self.pro_gains, self.pro_noise_ln_vars)\n",
    "        bx_ = bx_.t() # make a column vector\n",
    "        A = self.A(bx_) # after dynamics\n",
    "        P_ = A.mm(P).mm(A.t())+Q # P_ = APA^T+Q\n",
    "        if not is_pos_def(P_):\n",
    "            print(\"P_:\", P_)\n",
    "            print(\"P:\", P)\n",
    "            print(\"A:\", A)\n",
    "            APA = A.mm(P).mm(A.t())\n",
    "            print(\"APA:\", APA)\n",
    "            print(\"APA +:\", is_pos_def(APA))\n",
    "        error = ox - self.observations(bx_)\n",
    "        S = H.mm(P_).mm(H.t()) + R # S = HPH^T+R\n",
    "        K = P_.mm(H.t()).mm(torch.inverse(S)) # K = PHS^-1\n",
    "        bx = bx_ + K.matmul(error)\n",
    "        I_KH = I - K.mm(H)\n",
    "        P = I_KH.mm(P_)\n",
    "\n",
    "        if not is_pos_def(P):\n",
    "            print(\"here\")\n",
    "            print(\"P:\", P)\n",
    "            P = (P + P.t()) / 2 + 1e-6 * I  # make symmetric to avoid computational overflows\n",
    "\n",
    "        bx = bx.t() #return to a row vector\n",
    "        b = bx.view(-1), P  # belief\n",
    "\n",
    "\n",
    "        # terminal check\n",
    "        terminal = self._isTerminal(bx, a) # check the monkey stops or not\n",
    "        return b, {'stop': terminal}\n",
    "\n",
    "\n",
    "    def observations(self, x): # observation noise on\n",
    "\n",
    "        on = torch.sqrt(torch.exp(self.obs_noise_ln_vars)) * torch.randn(2).cuda() # random generation of observation noise\n",
    "        vel, ang_vel = torch.split(x.view(-1),1)[-2:]\n",
    "\n",
    "        ovel = self.obs_gains[0] * vel + on[0]\n",
    "        oang_vel = self.obs_gains[1] * ang_vel + on[1]\n",
    "        ox = torch.stack((ovel, oang_vel))\n",
    "        return ox\n",
    "\n",
    "    def observations_mean(self, x): # observation without noise\n",
    "\n",
    "        vel, ang_vel = torch.split(x.view(-1),1)[-2:]\n",
    "\n",
    "        ovel = self.obs_gains[0] * vel\n",
    "        oang_vel = self.obs_gains[1] * ang_vel\n",
    "        ox = torch.stack((ovel, oang_vel))\n",
    "        return ox\n",
    "\n",
    "    def A(self, x_): # F in wiki\n",
    "        dt = self.dt\n",
    "        px, py, ang, vel, ang_vel = torch.split(x_.view(-1),1)\n",
    "\n",
    "        A_ = torch.zeros(5, 5).cuda()\n",
    "        A_[:3, :3] = torch.eye(3)\n",
    "        A_[0, 2] = - vel * torch.sin(ang) * dt\n",
    "        A_[1, 2] = vel * torch.cos(ang) * dt\n",
    "        return A_\n",
    "\n",
    "    def Breshape(self, b, time, theta): # reshape belief for policy\n",
    "        pro_gains, pro_noise_ln_vars, obs_gains, obs_noise_ln_vars, goal_radius = theta\n",
    "        x, P = b\n",
    "        px, py, ang, vel, ang_vel = torch.split(x.view(-1), 1)\n",
    "        r = torch.norm(torch.cat([px, py])).view(-1)\n",
    "        rel_ang = ang - torch.atan2(-py, -px).view(-1)\n",
    "        rel_ang = range_angle(rel_ang)\n",
    "        vecL = vectorLowerCholesky(P)\n",
    "        state = torch.cat([r, rel_ang, vel, ang_vel, time, vecL, pro_gains.view(-1), \n",
    "                           pro_noise_ln_vars.view(-1), obs_gains.view(-1), obs_noise_ln_vars.view(-1), \n",
    "                           torch.ones(1).cuda()*goal_radius]) # original\n",
    "\n",
    "        return state.view(1, -1)\n",
    "\n",
    "    def _isTerminal(self, x, a, log=True):\n",
    "        terminal_vel = self.terminal_vel\n",
    "        terminal = is_terminal_action(a, terminal_vel)\n",
    "        return terminal.item() == 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.SEED_NUMBER = 0\n",
    "\n",
    "        self.WORLD_SIZE = 1.0\n",
    "        self.ACTION_DIM = 2\n",
    "        self.STATE_DIM = 29\n",
    "\n",
    "        self.TERMINAL_VEL = torch.tensor(0.1).cuda()  # norm(action) that you believe as a signal to stop 0.1\n",
    "        # all times are in second\n",
    "        self.DELTA_T = torch.tensor(1).cuda()  # time to perform one action\n",
    "        self.EPISODE_TIME = torch.tensor(70).cuda()\n",
    "        self.EPISODE_LEN = self.EPISODE_TIME / self.DELTA_T  # number of time steps(actions) for one episode\n",
    "\n",
    "        self.TOT_T = 2000000000  # total number of time steps for this code\n",
    "\n",
    "        self.BATCH_SIZE = 512  # for replay memory\n",
    "        self.REWARD = torch.tensor(10).cuda()  # for max reward\n",
    "        self.DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "        self.STD_STEP_SIZE = 2e-5  # 1e-4 action space noise (default: 2e-3)\n",
    "\n",
    "        self.filename = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.data_path = './'\n",
    "\n",
    "        self.goal_radius_range = torch.tensor([0.10* self.WORLD_SIZE, 0.2* self.WORLD_SIZE]).cuda() #0.175: best radius\n",
    "        self.GOAL_RADIUS_STEP_SIZE = torch.tensor(1e-5).cuda()\n",
    "\n",
    "        self.initial_radius_range = [0.25, self.WORLD_SIZE] # [0.25,1] = [100,400] cm\n",
    "        self.relative_angle_range = torch.tensor([-40/180*pi,40/180*pi]).cuda()  # in real experiment, the relative angle range is [-40,40]\n",
    "        self.gains_range = torch.tensor([0.025, 0.1, pi/40, pi/10]).cuda() # [100cm/s,400cm/s,45deg/s,180deg/s]\n",
    "        self.noise_range = torch.tensor([np.log(0.025/100), np.log(0.025/5), np.log((pi/40)/100), np.log((pi/40)/5)]).cuda() #SNR 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionNoise(object): # Gaussian\n",
    "    def __init__(self, action_dim, mean=torch.tensor(0).cuda(), std=torch.tensor(1).cuda()):\n",
    "        self.mu = torch.ones(action_dim).cuda() * mean\n",
    "        self.scale = std\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "    def reset(self, mean, std):\n",
    "        self.mu = torch.ones(self.action_dim).cuda() * mean\n",
    "        self.scale = std\n",
    "\n",
    "    def noise(self):\n",
    "        n = torch.randn(2).cuda()\n",
    "        return self.mu + self.scale*n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "class Agent():\n",
    "    def __init__(self, input_dim, action_dim, arg, filename=None, hidden_dim=128, gamma=0.99, tau=0.001, memory_size=1e6,\n",
    "                 device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")):\n",
    "\n",
    "        self.device = device\n",
    "        self.input_dim = input_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.data_path = arg.data_path\n",
    "\n",
    "        print(\"Running DDPG Agent: using \", self.device)\n",
    "\n",
    "        self.actor = Actor(input_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.target_actor = Actor(input_dim, action_dim, hidden_dim).to(self.device)  # target NW\n",
    "        self.critic = Critic(input_dim, action_dim, hidden_dim).to(self.device)\n",
    "        self.target_critic = Critic(input_dim, action_dim, hidden_dim).to(self.device)# target NW\n",
    "\n",
    "\n",
    "        self.actor_optim = Adam(self.actor.parameters(), lr=1e-4) # \n",
    "        self.critic_optim = Adam(self.critic.parameters(), lr=1e-3)\n",
    "\n",
    "        self.priority = False\n",
    "        self.memory = ReplayMemory(int(memory_size), priority=self.priority)\n",
    "\n",
    "        self.args = (input_dim, action_dim, hidden_dim)\n",
    "        hard_update(self.target_actor, self.actor)  # Make sure target is with the same weight\n",
    "        hard_update(self.target_critic, self.critic)\n",
    "        self.create_save_file(filename)\n",
    "\n",
    "\n",
    "        # for belief step\n",
    "        self.Bstep = BeliefStep(arg).cuda()\n",
    "\n",
    "    def select_action(self,  state, action_noise=None, param = None):\n",
    "\n",
    "        if param is not None:\n",
    "            mu = self.actor_perturbed(state).detach()\n",
    "        else: # no parameter space noise\n",
    "            mu = self.actor(state).detach()\n",
    "\n",
    "        if action_noise is not None:\n",
    "            mu += action_noise.noise()\n",
    "        else:\n",
    "            mu = mu\n",
    "        return mu.clamp(-1, 1)\n",
    "\n",
    "    def update_parameters(self, batch):\n",
    "        states = torch.cat(batch.state)\n",
    "        next_states = torch.cat(batch.next_state)\n",
    "        actions = torch.cat(batch.action)\n",
    "        rewards = torch.cat(batch.reward).unsqueeze(1)\n",
    "        dones = torch.cat(batch.done)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.target_actor(next_states) # use target\n",
    "            next_qvalues = self.target_critic(next_states, next_actions) # use target network\n",
    "            #next_qvalues = self.target_critic(next_states, next_actions) * (1 - dones)\n",
    "            target_qvalues = rewards + self.gamma * next_qvalues\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        pred_qvalues = self.critic(states, actions)\n",
    "        value_loss = torch.mean((pred_qvalues - target_qvalues)**2)\n",
    "        value_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        self.actor_optim.zero_grad()\n",
    "        policy_loss = -self.critic(states, self.actor(states))\n",
    "        policy_loss = policy_loss.mean()\n",
    "        policy_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "        return policy_loss, value_loss\n",
    "\n",
    "    def learn(self, batch_size=512):\n",
    "            # sample new batch here\n",
    "        batch, _ = self.memory.sample(batch_size)\n",
    "        losses = self.update_parameters(batch)\n",
    "        soft_update(self.target_actor, self.actor, self.tau)\n",
    "        soft_update(self.target_critic, self.critic, self.tau)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def save(self, filename, episode):\n",
    "        state = {\n",
    "            'args': self.args,\n",
    "            'actor_dict': self.actor.state_dict(),\n",
    "            'critic_dict': self.critic.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(state, self.file)\n",
    "\n",
    "    def load(self, filename):\n",
    "        file = self.data_path +'trained_agent/'+filename+'.pth.tar'\n",
    "        state = torch.load(file, map_location=lambda storage, loc: storage)\n",
    "        if self.args != state['args']:\n",
    "            print('Agent parameters from file are different from call')\n",
    "            print('Overwriting agent to load file ... ')\n",
    "            args = state['args']\n",
    "            #self = Agent(*args)\n",
    "            self.__init__(*args)\n",
    "\n",
    "        self.actor.load_state_dict(state['actor_dict'])\n",
    "        self.critic.load_state_dict(state['critic_dict'])\n",
    "        hard_update(self.target_actor, self.actor)  # Make sure target is with the same weight\n",
    "        hard_update(self.target_critic, self.critic)\n",
    "        #print('Loaded')\n",
    "        return\n",
    "\n",
    "    def create_save_file(self, filename):\n",
    "        path = self.data_path+'trained_agent'\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        if filename == None:\n",
    "            self.file = next_path(path + '/' + 'ddpgmodel_%s.pth.tar')\n",
    "        else: self.file = path + '/' + filename + '.pth.tar'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# firefly Env Model\n",
    "\"\"\"\n",
    "This is the main description for firefly task model\n",
    "This code is for environment\n",
    "\n",
    "This code uses the polar coordinate\n",
    "next_x = torch.stack((vel, ang_vel, r, ang))\n",
    "state = torch.cat([vel, ang_vel, r, ang, vecL, time]) # for policy network\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, arg):\n",
    "        super(self.__class__, self).__init__()\n",
    "        # constants\n",
    "        self.dt = arg. DELTA_T\n",
    "        self.action_dim = arg.ACTION_DIM\n",
    "        self.state_dim = arg.STATE_DIM\n",
    "        self.box = arg.WORLD_SIZE #initial value\n",
    "        self.max_goal_radius = arg.goal_radius_range[0]\n",
    "        self.GOAL_RADIUS_STEP = arg.GOAL_RADIUS_STEP_SIZE\n",
    "        #self.rendering = Render()\n",
    "        #self.reset()\n",
    "\n",
    "    def reset(self, gains_range, noise_range, goal_radius_range,relative_angle_range,initial_radius_range=None, \n",
    "              goal_radius=None, pro_gains=None, pro_noise_ln_vars=None):\n",
    "\n",
    "        self.pro_gains = torch.zeros(2).cuda()\n",
    "        self.pro_noise_ln_vars = torch.zeros(2).cuda()\n",
    "\n",
    "\n",
    "        if pro_gains is None:\n",
    "            self.pro_gains[0] = torch.zeros(1).uniform_(gains_range[0], gains_range[1])  #[proc_gain_vel]\n",
    "            self.pro_gains[1] = torch.zeros(1).uniform_(gains_range[2],\n",
    "                                                        gains_range[3])  # [proc_gain_ang]\n",
    "        else:\n",
    "            self.pro_gains = pro_gains\n",
    "\n",
    "        if pro_noise_ln_vars is None:\n",
    "\n",
    "            self.pro_noise_ln_vars[0] = -1 * sample_exp(-noise_range[1], -noise_range[0]) #[proc_vel_noise]\n",
    "            self.pro_noise_ln_vars[1] = -1 * sample_exp(-noise_range[3], -noise_range[2]) #[proc_ang_noise]\n",
    "        else:\n",
    "            self.pro_noise_ln_vars = pro_noise_ln_vars\n",
    "\n",
    "        if goal_radius is None:\n",
    "            #self.max_goal_radius = min(self.max_goal_radius + self.GOAL_RADIUS_STEP, goal_radius_range[1])\n",
    "            #self.goal_radius = torch.zeros(1).uniform_(goal_radius_range[0], self.max_goal_radius).cuda()\n",
    "            self.goal_radius = torch.zeros(1).uniform_(goal_radius_range[0], goal_radius_range[1]).cuda()\n",
    "        else:\n",
    "            self.goal_radius = goal_radius\n",
    "\n",
    "\n",
    "        self.time = torch.zeros(1)\n",
    "        if initial_radius_range is None:\n",
    "            min_r = self.goal_radius.item()\n",
    "            r = torch.sqrt(torch.zeros(1).uniform_(min_r**2, self.box**2))  # sample radius uniformly in 2D\n",
    "            self.r = r\n",
    "        else:\n",
    "            r = torch.sqrt(torch.zeros(1).uniform_(initial_radius_range[0]**2, initial_radius_range[1]**2))\n",
    "            self.r = r\n",
    "\n",
    "        loc_ang = torch.zeros(1).uniform_(-pi, pi) # location angel: to determine initial location\n",
    "        px = r * torch.cos(loc_ang)\n",
    "        py = r * torch.sin(loc_ang)\n",
    "        rel_ang = torch.zeros(1).uniform_(relative_angle_range[0], relative_angle_range[1])\n",
    "        ang = rel_ang + loc_ang + pi # heading angle of monkey, pi is added in order to make the monkey toward firefly\n",
    "        ang = range_angle(ang)\n",
    "\n",
    "        vel = torch.zeros(1)\n",
    "        ang_vel = torch.zeros(1)\n",
    "\n",
    "        x = torch.cat([px, py, ang, vel, ang_vel]).cuda()\n",
    "        return x, self.pro_gains, self.pro_noise_ln_vars, self.goal_radius\n",
    "\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        # get a real next state of monkey\n",
    "        next_x = dynamics(x, a, self.dt, self.box, self.pro_gains, self.pro_noise_ln_vars)\n",
    "        pos = next_x.view(-1)[:2] # x and y position\n",
    "        reached_target = (torch.norm(pos) <= self.goal_radius)\n",
    "\n",
    "        return next_x, reached_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
